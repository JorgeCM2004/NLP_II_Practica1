{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d20da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.genre_classification.F_Basic_Models import Basic_Models\n",
    "from src.genre_classification.F_Dataset_Downloader import Dataset_Downloader\n",
    "from src.genre_classification.F_Pretrained_models import Pretrained\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169589fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'biography', 'sci-fi', 'sport', 'short', 'adult', 'news', 'musical', 'action', 'animation', 'comedy', 'talk-show', 'thriller', 'history', 'film-noir', 'horror', 'adventure', 'war', 'mystery', 'game-show', 'music', 'fantasy', 'reality-tv', 'western', 'drama', 'romance', 'crime', 'documentary', 'family'}\n"
     ]
    }
   ],
   "source": [
    "dataset_downloader = Dataset_Downloader()\n",
    "train_path, test_path = dataset_downloader()\n",
    "\n",
    "train_data = pd.read_csv(train_path.resolve())\n",
    "test_data = pd.read_csv(test_path.resolve())\n",
    "\n",
    "train_data.columns\n",
    "\n",
    "print(set(train_data['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_data.drop(columns=[\"genre\"]), train_data[\"genre\"]\n",
    "\n",
    "x_test, y_test = test_data.drop(columns=[\"genre\"]), test_data[\"genre\"]\n",
    "\n",
    "unique_labels = sorted(list(set(train_data[\"genre\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546fe63",
   "metadata": {},
   "source": [
    "# Pruebas de Modelos Basicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha terminado el preprocesado\n",
      "Ha terminado la transfromacion al embbeding\n",
      "Ha terminado de entrenar\n",
      "Ha terminado de preprocesar entrenamiento\n",
      "Ha terminado de representar entrenamiento\n",
      "(55400,) (55400,)\n"
     ]
    }
   ],
   "source": [
    "basic_models_name = ['Random_Forest', 'Naive_Bayes', 'LogReg', 'Linear_SVM']\n",
    "\n",
    "for model_name in basic_models_name:\n",
    "    model = Basic_Models(model_type= model_name)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_hat = model.transform(x_test)\n",
    "    model.evaluate(y_true= y_test, y_hat= y_hat, labels = set(y_test), evaluate_type= \"all_metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6ba7e",
   "metadata": {},
   "source": [
    "# Pruebas de Modelos Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb81ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(label_str, num_classes, label_map):\n",
    "    vector = [0] * num_classes\n",
    "    if label_str in label_map:\n",
    "        idx = label_map[label_str]\n",
    "        vector[idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ed733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_data[\"text\"].tolist() \n",
    "test_texts = test_data[\"text\"].tolist()   \n",
    "\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "train_labels_one_hot = [to_one_hot(l, len(id2label), label2id) for l in train_data[\"genre\"]]\n",
    "\n",
    "val = Pretrained(model_type=\"TOCHO\", id2label=id2label) \n",
    "\n",
    "val.fit(train_texts, train_labels_one_hot)\n",
    "\n",
    "predictions = val.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6045940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando b√∫squeda de hiperpar√°metros (GridSearch)...\n",
      "Esto puede tardar unos minutos, pero encontrar√° la mejor configuraci√≥n matem√°tica.\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 3. Entrenar con b√∫squeda autom√°tica\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Nota: Pasamos X_train y y_train juntos en un DF temporal para la funci√≥n\u001b[39;00m\n\u001b[32m     97\u001b[39m df_train_subset = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: X_train, \u001b[33m'\u001b[39m\u001b[33mlabel_reduced\u001b[39m\u001b[33m'\u001b[39m: y_train})\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m best_model = \u001b[43mtrain_optimized_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# 4. Evaluar el modelo GANADOR\u001b[39;00m\n\u001b[32m    101\u001b[39m preds = best_model.predict(X_val)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mtrain_optimized_classifier\u001b[39m\u001b[34m(train_df, text_col, label_col)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# C. Configuramos el √°rbitro (GridSearchCV)\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# cv=3: Validaci√≥n cruzada de 3 pliegues (entrena 3 veces por cada combinaci√≥n)\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# scoring='f1_weighted': Buscamos maximizar el equilibrio entre precisi√≥n y recall\u001b[39;00m\n\u001b[32m     64\u001b[39m search = GridSearchCV(\n\u001b[32m     65\u001b[39m     pipeline, \n\u001b[32m     66\u001b[39m     param_grid, \n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ ¬°Mejor configuraci√≥n encontrada!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMejores par√°metros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Definici√≥n del Mapeo\n",
    "label_mapping = {\n",
    "    # --- GRUPO 0: Action & Suspense ---\n",
    "    \"action\": 0, \"adventure\": 0, \"crime\": 0, \"fantasy\": 0,\n",
    "    \"mystery\": 0, \"sci-fi\": 0, \"thriller\": 0, \"war\": 0, \"western\": 0,\n",
    "\n",
    "    # --- GRUPO 1: Drama & Serious ---\n",
    "    \"biography\": 1, \"drama\": 1, \"film-noir\": 1, \"history\": 1, \"romance\": 1,\n",
    "\n",
    "    # --- GRUPO 2: Comedy & Light ---\n",
    "    \"animation\": 2, \"comedy\": 2, \"family\": 2, \"music\": 2, \"musical\": 2,\n",
    "\n",
    "    # --- GRUPO 3: Factual & TV ---\n",
    "    \"documentary\": 3, \"game-show\": 3, \"news\": 3, \"reality-tv\": 3,\n",
    "    \"sport\": 3, \"talk-show\": 3,\n",
    "\n",
    "    # --- GRUPO 4: Niche / Distinct ---\n",
    "    \"adult\": 4, \"horror\": 4, \"short\": 4\n",
    "}\n",
    "\n",
    "# Nombres para el reporte\n",
    "super_class_names = [\n",
    "    \"Action_Suspense\", \n",
    "    \"Drama_Serious\", \n",
    "    \"Comedy_Family\", \n",
    "    \"Factual_TV\", \n",
    "    \"Niche_Horror\"\n",
    "]\n",
    "\n",
    "def prepare_data(df, text_col='text', label_col='genre'):\n",
    "    df = df.copy()\n",
    "    # CORRECCI√ìN 1: Usar 'label_mapping' en lugar de 'reduction_map'\n",
    "    df['label_reduced'] = df[label_col].map(label_mapping)\n",
    "    \n",
    "    # Limpieza extra: Eliminar filas que no se hayan podido mapear (por si acaso)\n",
    "    df = df.dropna(subset=['label_reduced'])\n",
    "    df['label_reduced'] = df['label_reduced'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_genre_classifier(train_df, text_col='text', label_col='label_reduced'):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=20000,    # Subimos de 10k a 20k para captar m√°s vocabulario √∫til\n",
    "            ngram_range=(1, 3),    # Unigramas y Bigramas (se mantiene)\n",
    "            stop_words='english',\n",
    "            min_df=5,              # NUEVO: Ignora palabras que aparecen en <5 docs (quita ruido)\n",
    "            sublinear_tf=True      \n",
    "        )),\n",
    "        ('clf', LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='saga',         \n",
    "            #class_weight='balanced', # VER SI CAMBIAR ESTE PARAMETRO\n",
    "            C=2.0,                \n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    print(\"Entrenando modelo OPTIMIZADO...\")\n",
    "    pipeline.fit(train_df[text_col], train_df[label_col])\n",
    "    return pipeline\n",
    "    \n",
    "\n",
    "\n",
    "if 'train_data' in locals():\n",
    "    # Preprocesar\n",
    "    df_clean = prepare_data(train_data)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "         df_clean['text'], \n",
    "         df_clean['label_reduced'], \n",
    "         test_size=0.2, \n",
    "         stratify=df_clean['label_reduced'],\n",
    "         random_state=42 # Recomendado para reproducibilidad\n",
    "     )\n",
    "\n",
    "    # Entrenar\n",
    "    model = train_genre_classifier(pd.DataFrame({'text': X_train, 'label_reduced': y_train}))\n",
    "\n",
    "    # Evaluar\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    # CORRECCI√ìN 2: Usar 'super_class_names'\n",
    "    print(classification_report(y_val, preds, target_names=super_class_names))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Error: La variable 'train_data' no est√° definida. Carga tu CSV primero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ed48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesando datos...\n",
      "Entrenando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\OneDrive\\Desktop\\Procesamiento del lenguaje natural II\\Practica 1 NLP II\\NLP_II_Practica1\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo con datos de Test...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Action_Suspense       0.70      0.86      0.77     29590\n",
      "  Drama_Serious       0.58      0.51      0.54     11125\n",
      "  Comedy_Family       0.49      0.25      0.33      4821\n",
      "     Factual_TV       0.73      0.65      0.69      3609\n",
      "   Niche_Horror       0.52      0.28      0.36      6255\n",
      "\n",
      "       accuracy                           0.66     55400\n",
      "      macro avg       0.60      0.51      0.54     55400\n",
      "   weighted avg       0.64      0.66      0.64     55400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Definici√≥n del Mapeo\n",
    "label_mapping = {\n",
    "    \"action\": 0, \"adventure\": 0, \"crime\": 0, \"fantasy\": 0, \"mystery\": 0, \"sci-fi\": 0, \"thriller\": 0, \"war\": 0, \"western\": 0,\n",
    "    \"biography\": 1, \"drama\": 1, \"film-noir\": 1, \"history\": 1, \"romance\": 1,\n",
    "    \"animation\": 2, \"comedy\": 2, \"family\": 2, \"music\": 2, \"musical\": 2,\n",
    "    \"documentary\": 3, \"game-show\": 3, \"news\": 3, \"reality-tv\": 3, \"sport\": 3, \"talk-show\": 3,\n",
    "    \"adult\": 4, \"horror\": 4, \"short\": 4\n",
    "}\n",
    "\n",
    "super_class_names = [\"Action_Suspense\", \"Drama_Serious\", \"Comedy_Family\", \"Factual_TV\", \"Niche_Horror\"]\n",
    "\n",
    "def prepare_data(df, text_col='text', label_col='genre'):\n",
    "    df = df.copy()\n",
    "    df['label_reduced'] = df[label_col].map(label_mapping)\n",
    "    df = df.dropna(subset=['label_reduced'])\n",
    "    df['label_reduced'] = df['label_reduced'].astype(int)\n",
    "    return df\n",
    "\n",
    "def train_genre_classifier(train_df, text_col='text', label_col='label_reduced'):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('clf', LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='saga',\n",
    "            class_weight='balanced', \n",
    "            C=1,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    print(\"Entrenando modelo...\")\n",
    "    pipeline.fit(train_df[text_col], train_df[label_col])\n",
    "    return pipeline\n",
    "\n",
    "# --- BLOQUE DE EJECUCI√ìN CORREGIDO ---\n",
    "\n",
    "# Verificamos que tengas ambos datasets cargados\n",
    "if 'train_data' in locals() and 'test_data' in locals():\n",
    "    \n",
    "    # 1. Preprocesar ambos\n",
    "    print(\"Preprocesando datos...\")\n",
    "    df_train_clean = prepare_data(train_data)\n",
    "    df_test_clean = prepare_data(test_data)\n",
    "\n",
    "    # 2. Definir X e y para ENTRENAMIENTO\n",
    "    X_train = df_train_clean['text']\n",
    "    y_train = df_train_clean['label_reduced']\n",
    "\n",
    "    # 3. Definir X e y para VALIDACI√ìN (Usamos el test set)\n",
    "    X_test = df_test_clean['text']\n",
    "    y_test = df_test_clean['label_reduced']\n",
    "\n",
    "    # 4. Entrenar (Pasamos los datos de train limpios)\n",
    "    # Creamos un DF temporal porque tu funci√≥n espera columnas con nombre\n",
    "    train_subset = pd.DataFrame({'text': X_train, 'label_reduced': y_train})\n",
    "    model = train_genre_classifier(train_subset)\n",
    "\n",
    "    # 5. Evaluar (Usamos los datos de test)\n",
    "    print(\"\\nEvaluando modelo con datos de Test...\")\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, preds, target_names=super_class_names))\n",
    "\n",
    "elif 'train_data' in locals():\n",
    "    # CASO: Si SOLO tienes train_data y no tienes test_data\n",
    "    print(\"‚ö†Ô∏è No se detect√≥ 'test_data', dividiendo train_data para validar...\")\n",
    "    df_clean = prepare_data(train_data)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "         df_clean['text'], \n",
    "         df_clean['label_reduced'], \n",
    "         test_size=0.2, \n",
    "         stratify=df_clean['label_reduced'],\n",
    "         random_state=42 \n",
    "     )\n",
    "    \n",
    "    train_subset = pd.DataFrame({'text': X_train, 'label_reduced': y_train})\n",
    "    model = train_genre_classifier(train_subset)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    print(classification_report(y_val, preds, target_names=super_class_names))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Error: Carga tus CSVs en las variables 'train_data' y 'test_data' primero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b04d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preparando datos...\n",
      "2. Aplicando balanceo 50/50 al set de ENTRENAMIENTO...\n",
      "   Original -> Action: 118362, Drama: 44497\n",
      "   BALANCEADO -> Action: 44497, Drama: 44497\n",
      "Entrenando modelo...\n",
      "\n",
      "--- RESULTADOS BINARIOS (Entrenamiento 50/50 estricto) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Action_Suspense       0.91      0.79      0.84     29590\n",
      "  Drama_Serious       0.58      0.78      0.67     11125\n",
      "\n",
      "       accuracy                           0.79     40715\n",
      "      macro avg       0.74      0.78      0.75     40715\n",
      "   weighted avg       0.82      0.79      0.79     40715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample # Importante para re-muestrear\n",
    "\n",
    "# 1. Definici√≥n del Mapeo\n",
    "label_mapping = {\n",
    "    \"action\": 0, \"adventure\": 0, \"crime\": 0, \"fantasy\": 0, \"mystery\": 0, \"sci-fi\": 0, \"thriller\": 0, \"war\": 0, \"western\": 0,\n",
    "    \"biography\": 1, \"drama\": 1, \"film-noir\": 1, \"history\": 1, \"romance\": 1,\n",
    "    \"animation\": 2, \"comedy\": 2, \"family\": 2, \"music\": 2, \"musical\": 2,\n",
    "    \"documentary\": 3, \"game-show\": 3, \"news\": 3, \"reality-tv\": 3, \"sport\": 3, \"talk-show\": 3,\n",
    "    \"adult\": 4, \"horror\": 4, \"short\": 4\n",
    "}\n",
    "\n",
    "binary_class_names = [\"Action_Suspense\", \"Drama_Serious\"]\n",
    "\n",
    "def prepare_data_binary(df, text_col='text', label_col='genre'):\n",
    "    df = df.copy()\n",
    "    df['label_reduced'] = df[label_col].map(label_mapping)\n",
    "    # Filtro: Solo Action(0) y Drama(1)\n",
    "    df = df[df['label_reduced'].isin([0, 1])]\n",
    "    df['label_reduced'] = df['label_reduced'].astype(int)\n",
    "    return df\n",
    "\n",
    "def force_exact_balance(df, label_col='label_reduced'):\n",
    "    \"\"\"\n",
    "    Esta funci√≥n recorta la clase mayoritaria para que tenga \n",
    "    EXACTAMENTE el mismo n√∫mero de filas que la minoritaria.\n",
    "    \"\"\"\n",
    "    # Separamos los dos grupos\n",
    "    df_action = df[df[label_col] == 0]\n",
    "    df_drama = df[df[label_col] == 1]\n",
    "    \n",
    "    # Contamos cu√°ntos hay de cada uno\n",
    "    n_action = len(df_action)\n",
    "    n_drama = len(df_drama)\n",
    "    \n",
    "    # Buscamos el m√≠nimo (normalmente ser√° Drama)\n",
    "    min_samples = min(n_action, n_drama)\n",
    "    \n",
    "    print(f\"   Original -> Action: {n_action}, Drama: {n_drama}\")\n",
    "    \n",
    "    # Recortamos ambos al tama√±o del m√°s peque√±o (Undersampling)\n",
    "    df_action_down = df_action.sample(n=min_samples, random_state=42)\n",
    "    df_drama_down = df_drama.sample(n=min_samples, random_state=42) # Si es el menor, se queda igual\n",
    "    \n",
    "    # Juntamos y mezclamos (shuffle)\n",
    "    df_balanced = pd.concat([df_action_down, df_drama_down])\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   BALANCEADO -> Action: {len(df_action_down)}, Drama: {len(df_drama_down)}\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "def train_binary_classifier(train_df, text_col='text', label_col='label_reduced'):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('clf', LogisticRegression(\n",
    "            solver='saga',\n",
    "            # YA NO USAMOS 'balanced' AQU√ç. \n",
    "            # Los datos entran 50/50, as√≠ que dejamos que el modelo trate a todos por igual.\n",
    "            C=0.5,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    print(\"Entrenando modelo...\")\n",
    "    pipeline.fit(train_df[text_col], train_df[label_col])\n",
    "    return pipeline\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "\n",
    "if 'train_data' in locals() and 'test_data' in locals():\n",
    "    print(\"1. Preparando datos...\")\n",
    "    # Limpiamos (Aqu√≠ todav√≠a est√°n desbalanceados)\n",
    "    df_train_raw = prepare_data_binary(train_data)\n",
    "    df_test_raw = prepare_data_binary(test_data)\n",
    "\n",
    "    # 2. BALANCEO MANUAL SOLO EN TRAIN\n",
    "    print(\"2. Aplicando balanceo 50/50 al set de ENTRENAMIENTO...\")\n",
    "    df_train_balanced = force_exact_balance(df_train_raw)\n",
    "\n",
    "    # El Test NO se toca (queremos ver la realidad)\n",
    "    X_test = df_test_raw['text']\n",
    "    y_test = df_test_raw['label_reduced']\n",
    "    \n",
    "    # 3. Entrenar\n",
    "    model = train_binary_classifier(df_train_balanced)\n",
    "\n",
    "    # 4. Evaluar\n",
    "    print(\"\\n--- RESULTADOS BINARIOS (Entrenamiento 50/50 estricto) ---\")\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds, target_names=binary_class_names))\n",
    "\n",
    "elif 'train_data' in locals():\n",
    "    # Caso solo train: Dividimos primero, y luego balanceamos SOLO la parte de train\n",
    "    df_raw = prepare_data_binary(train_data)\n",
    "    \n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "         df_raw['text'], \n",
    "         df_raw['label_reduced'], \n",
    "         test_size=0.2, \n",
    "         stratify=df_raw['label_reduced'],\n",
    "         random_state=42 \n",
    "     )\n",
    "    \n",
    "    # Reconstruimos el DF de train para pasarlo por la balanceadora\n",
    "    df_train_temp = pd.DataFrame({'text': X_train_split, 'label_reduced': y_train_split})\n",
    "    \n",
    "    print(\"Aplicando balanceo 50/50 al split de ENTRENAMIENTO...\")\n",
    "    df_train_balanced = force_exact_balance(df_train_temp)\n",
    "    \n",
    "    model = train_binary_classifier(df_train_balanced)\n",
    "    \n",
    "    preds = model.predict(X_val_split)\n",
    "    print(\"\\n--- RESULTADOS BINARIOS (Entrenamiento 50/50 estricto) ---\")\n",
    "    print(classification_report(y_val_split, preds, target_names=binary_class_names))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Error: Carga tus datos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
